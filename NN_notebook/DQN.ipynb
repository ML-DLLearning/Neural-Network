{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148eea11",
   "metadata": {},
   "source": [
    "# DQN(Deep Q Learning)(RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1fad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "TARGET_REPLACE_ITER = 100\n",
    "MEMORY_CAPACITY = 2000\n",
    "env = gym.make('CartPole-v0')  # 通过 OpenAI Gym 创建一个名为 'CartPole-v0' 的环境实例\n",
    "env = env.unwrapped# 取消环境的限制，使其可以进行更多的操作\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6fec31",
   "metadata": {},
   "source": [
    "env.unwrapped：比如原始环境可能对步数有最大限制（如 200 步），解锁后可以通过自定义配置改变这个限制。  \n",
    "env.action_space.n：获取该环境中动作空间的维度（即动作的数量）。对于 CartPole-v0，动作空间是离散的，只有两个动作：0 表示将小车向左推；1 表示将小车向右推。因此，这里 N_ACTIONS 的值为 2  \n",
    "env.observation_space.shape[0]：获取该环境中状态空间的维度（即状态特征的数量）；CartPole-v0包含小车的位置；小车的速度；杆子的角度；杆子的角速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 10)#输入环境状态，输出隐藏层\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.out = nn.Linear(10, N_ACTIONS)#输出小车要采取的动作的价值\n",
    "        self.out.weight.data.normal_(0, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    #其实就是\n",
    "    #def forward(self, x):\n",
    "        # y = self.fc1(x)\n",
    "        # z = F.relu(y)\n",
    "        # actions_value = self.out(z)\n",
    "        # return actions_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed99038",
   "metadata": {},
   "source": [
    "使用正态分布（高斯分布）随机初始化第一层全连接层（fc1）的权重：  \n",
    "self.fc1：表示网络中的第一层全连接层。  \n",
    "self.fc1.weight：表示这一层的权重。  \n",
    "self.fc1.weight.data：表示获取这一层权重的底层数据。  \n",
    ".normal_(0, 0.1)：表示用均值为 0，标准差为 0.1 的正态分布随机填充数据。  \n",
    "\n",
    "初始化的权重是什么：nn.Linear 层的 W 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net,self.target_net = Net(),Net()#这是两个一模一样的网络\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))  # 经验回放记忆\n",
    "        self.memory_counter = 0\n",
    "        self.learn_step_counter = 0\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        #随机生成一个浮点数,默认范围[0, 1),用来决定是否选择当前的最优动作（利用）或随机动作（探索）。\n",
    "        if np.random.uniform() < EPSILON:  #greedy\n",
    "            actions_value = self.eval_net(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()[0,0] #[0,0]：提取二维数组中的具体值\n",
    "            \n",
    "        else: #随机选择动作\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "\n",
    "    #经验回放存储机制\n",
    "    def store_transition(self, s, a, r, s_):  #（当前状态，执行的动作，即使奖励，下一状态）\n",
    "    #s(), a标量（0或1）, r标量, s_()\n",
    "        transition = np.hstack((s, a, r, s_))  # 合并成一个一维数组:若 s 和 s_ 的维度为 4，a 为标量，r 为标量,那么transition 的形状为 (4,1,1,4,)\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition  # 经验池 self.memory 的第 index 行存储当前的 transition\n",
    "        self.memory_counter += 1  # 每存储一条新经验后，计数器 self.memory_counter 增加 1\n",
    "        \n",
    "    def learn(self):\n",
    "        #target_net的更新    不一步一更新\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:  # 每隔 TARGET_REPLACE_ITER 步更新一次目标网络\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())#将评估网络的参数复制到目标网络中，以更新目标网络的参数\n",
    "            #state_dict() 方法来获取模型的所有参数和缓冲区，本例中是nn.Linear 层的权重和偏置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e0c8b",
   "metadata": {},
   "source": [
    "***EPSILON:*** ε-贪心策略中的超参数；在 ε-贪心策略中，智能体会以概率 EPSILON 选择当前的最优动作（即利用策略），以概率 1 - EPSILON 随机选择一个动作（即探索新动作）；  \n",
    "作用：控制探索和利用的平衡，较大的 EPSILON 值，意味着智能体更倾向于选择当前的最优动作（利用）；较小的 EPSILON 值，意味着智能体更倾向于随机探索新动作（探索）。  \n",
    "\n",
    "***GAMMA:*** 折扣因子，决定智能体对未来奖励的重视程度。  \n",
    "强化学习中，目标是最大化未来的累积奖励。折扣因子用于控制未来奖励的重要性：  \n",
    "\n",
    "$$Q(s,a)=r+γ⋅maxQ(s′,a′)$$\n",
    "\n",
    "Q(s,a):状态s下执行动作a所能获得的长期累积奖励的期望  \n",
    "r:当前奖励  \n",
    "γ:折扣因子\n",
    "maxQ(s′,a′):下一状态的最大 Q 值  \n",
    "解释：当 GAMMA 接近 1 时，智能体会更加重视长期奖励；当 GAMMA 接近 0 时，智能体会更加重视当前的即时奖励。  \n",
    "\n",
    "***MEMORY_CAPACITY:*** 是经验池的容量，表示最多可以存储多少条经验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fda7b",
   "metadata": {},
   "source": [
    "两个网络一模一样：  \n",
    "self.eval_net（评价网络）：用来计算当前状态x下所有可能的动作的预测的 Q 值  \n",
    "每一步都在更新  \n",
    "self.target_net（目标网络）：  \n",
    "定期更新，由TARGET_REPLACE_ITER决定，详见def learn(self):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae228e",
   "metadata": {},
   "source": [
    "解释 $action = torch.max(actions\\_value, 1)[1].data.numpy()[0,0]:$   \n",
    "torch.max(input, dim) 用于在指定维度上，计算输入张量的最大值及其索引，返回一个元组 (values, indices)(最大值，索引)  \n",
    "[1] 是对 torch.max(actions_value, 1) 返回的元组 (values, indices) 中的第二个元素（indices）进行索引操作。也就是说，[1] 提取的是每一行最大值的索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d246d",
   "metadata": {},
   "source": [
    "#### shape的dim：  \n",
    "$actions_value = torch.tensor([[0.1, 0.5, 0.3],\n",
    "                              [0.8, 0.2, 0.4]])$  \n",
    "| Row (Batch)   | Action 1 | Action 2 | Action 3 |\n",
    "|---------------|----------|----------|----------|\n",
    "| Sample 1      | 0.1      | 0.5      | 0.3      |\n",
    "| Sample 2      | 0.8      | 0.2      | 0.4      |\n",
    "\n",
    "dim=0：表示按 列 计算最大值，即跨行（纵向）比较每一列的值。  \n",
    "dim=1：表示按 行 计算最大值，即跨列（横向）比较每一行的值。  \n",
    "那么 dim=1 表示我们希望“从每一行中取列的最大值”：\n",
    "第一行 [0.1, 0.5, 0.3] 中，最大值是 0.5，对应的列索引为 1。\n",
    "第二行 [0.8, 0.2, 0.4] 中，最大值是 0.8，对应的列索引为 0。\n",
    "\n",
    "actions_value的shape是(batch_size,num_actions)，也就是包含了前批次中所有状态下的所有动作分别对应的价值  \n",
    "本例中每一次训练都包含四个状态（小车的位置、小车的速度、杆子的角度、杆子的角速度），即actions_value包含了：只考虑小车位置时0，1动作对应的价值；只考虑小车速度时……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eed241",
   "metadata": {},
   "source": [
    "$def store_transition(self, s, a, r, s_):$  \n",
    "self.memory_counter经验计数器，在它小于MEMORY_CAPACITY时，index会因为取余操作而随memory_counter递增，然后将经验值transition存入第index行，最后计数+1。  \n",
    "当计数大于MEMORY_CAPACITY时，index重新遍历0~MEMORY_CAPACITY-1,覆盖掉self.memory中的旧数据，所以说MEMORY_CAPACITY是经验池的流量。  \n",
    "为什么不设置成∞：内存有限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8230aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
